{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp # type: ignore\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "from sklearn.cluster import DBSCAN\n",
    "import json\n",
    "\n",
    "import cv2\n",
    "from fer import FER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttributeError: module 'ml_dtypes' has no attribute 'float8_e3m4'\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1295: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 126\u001b[0m\n\u001b[0;32m    123\u001b[0m out \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoWriter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_video.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m, fourcc, \u001b[38;5;241m20.0\u001b[39m, (\u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m480\u001b[39m))\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Processando o vídeo\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_video\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 117\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(input_video)\u001b[0m\n\u001b[0;32m    115\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    116\u001b[0m out\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m--> 117\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdestroyAllWindows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1295: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\n"
     ]
    }
   ],
   "source": [
    "# Função para gerar um nome aleatório para as pessoas\n",
    "def generate_person_name(person_id):\n",
    "    return f\"person_{person_id}\"\n",
    "\n",
    "# Função para detectar rostos e emoções\n",
    "def detect_faces_and_emotions(frame, face_cascade, emotion_detector, person_counter):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    faces_data = []\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y + h, x:x + w]\n",
    "        emotion = emotion_detector.top_emotion(face)\n",
    "        person_name = generate_person_name(person_counter)\n",
    "        \n",
    "        # Salvando o rosto detectado com nome aleatório\n",
    "        cv2.imwrite(f\"faces/{person_name}.jpg\", face)\n",
    "        \n",
    "        # Armazenar dados da pessoa\n",
    "        faces_data.append({\n",
    "            \"name\": person_name,\n",
    "            \"emotion\": emotion[0],  # A emoção detectada\n",
    "            \"coords\": (x, y, w, h)\n",
    "        })\n",
    "        \n",
    "        # Desenhando um retângulo ao redor do rosto\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, person_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "\n",
    "    return frame, faces_data\n",
    "\n",
    "# Função para detectar atividades e anomalias\n",
    "def detect_anomalies(frame, prev_frame):\n",
    "    # Lógica para detecção de movimento anômalo (ex: diferença entre frames)\n",
    "    motion_detected = False\n",
    "    diff = cv2.absdiff(frame, prev_frame)\n",
    "    gray_diff = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray_diff, 50, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    if cv2.countNonZero(thresh) > 500:  # Movimentos significativos\n",
    "        motion_detected = True\n",
    "\n",
    "    return motion_detected\n",
    "\n",
    "# Função para processar o vídeo e gerar relatório\n",
    "def process_video(input_video):\n",
    "    cap = cv2.VideoCapture(input_video)\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    emotion_detector = FER()\n",
    "    person_counter = 1\n",
    "    persons = {}\n",
    "\n",
    "    prev_frame = None\n",
    "    frame_number = 0\n",
    "    report = {\"persons\": []}\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_number += 1\n",
    "        # Detecção de rostos e emoções\n",
    "        frame, faces_data = detect_faces_and_emotions(frame, face_cascade, emotion_detector, person_counter)\n",
    "        \n",
    "        # Detecção de anomalias\n",
    "        if prev_frame is not None:\n",
    "            motion_detected = detect_anomalies(frame, prev_frame)\n",
    "            if motion_detected:\n",
    "                # Marcar anomalia no vídeo\n",
    "                cv2.putText(frame, 'Anomaly Detected', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # Atualizando os dados no relatório\n",
    "        for person in faces_data:\n",
    "            name = person[\"name\"]\n",
    "            if name not in persons:\n",
    "                persons[name] = {\n",
    "                    \"name\": name,\n",
    "                    \"movements\": [],\n",
    "                    \"anomalies\": [],\n",
    "                    \"emotion\": []\n",
    "                }\n",
    "            \n",
    "            # Adicionando movimentos e emoções no relatório\n",
    "            persons[name][\"emotion\"].append({\n",
    "                \"frame\": frame_number,\n",
    "                \"emotion_name\": person[\"emotion\"]\n",
    "            })\n",
    "            \n",
    "            if motion_detected:\n",
    "                persons[name][\"anomalies\"].append({\n",
    "                    \"frame\": frame_number,\n",
    "                    \"anomalie_name\": \"Movement Anomaly\"\n",
    "                })\n",
    "        \n",
    "        prev_frame = frame.copy()\n",
    "\n",
    "        # Exibindo o vídeo com as demarcações\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "        # Salvando vídeo com as demarcações\n",
    "        out.write(frame)\n",
    "\n",
    "        # Pressione 'q' para sair\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Gerar relatório\n",
    "    report[\"persons\"] = list(persons.values())\n",
    "    \n",
    "    # Salvar relatório em um arquivo JSON\n",
    "    with open(\"report.json\", \"w\") as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_video = \"video_entrada.mp4\"\n",
    "    # Definindo o arquivo de saída com demarcações\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter('output_video.mp4', fourcc, 20.0, (640, 480))\n",
    "    \n",
    "    # Processando o vídeo\n",
    "    process_video(input_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25-02-01 14:10:47 - facial_expression_model_weights.h5 will be downloaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://github.com/serengil/deepface_models/releases/download/v1.0/facial_expression_model_weights.h5\n",
      "To: C:\\Users\\mathe\\.deepface\\weights\\facial_expression_model_weights.h5\n",
      "100%|██████████| 5.98M/5.98M [00:00<00:00, 35.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processamento completo. Relatório gerado e vídeo de saída salvo.\n",
      "Relatório:\n",
      "Total de frames analisados: 3326\n",
      "Número de anomalias detectadas: 18884\n",
      "Emoções detectadas: ['fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'fear', 'sad', 'fear', 'sad', 'fear', 'fear', 'fear', 'fear', 'fear', 'happy', 'happy', 'fear', 'neutral', 'fear', 'sad', 'neutral', 'fear', 'fear', 'sad', 'fear', 'fear', 'happy', 'happy', 'fear', 'happy', 'fear', 'happy', 'sad', 'sad', 'happy', 'happy', 'sad', 'happy', 'happy', 'surprise', 'happy', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'neutral', 'angry', 'angry', 'sad', 'sad', 'sad', 'sad', 'fear', 'happy', 'happy', 'neutral', 'happy', 'neutral', 'happy', 'neutral', 'happy', 'neutral', 'happy', 'neutral', 'happy', 'happy', 'neutral', 'neutral', 'happy', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'happy', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'happy', 'happy', 'neutral', 'neutral', 'neutral', 'happy', 'neutral', 'neutral', 'neutral', 'neutral', 'happy', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'happy', 'happy', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'happy', 'happy', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'happy', 'neutral', 'happy', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'happy', 'neutral', 'neutral', 'neutral', 'happy', 'neutral', 'neutral', 'happy', 'happy', 'neutral', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'neutral', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'happy', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'happy', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'fear', 'surprise', 'neutral', 'happy', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'fear', 'surprise', 'sad', 'fear', 'surprise', 'surprise', 'fear', 'surprise', 'fear', 'surprise', 'fear', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'neutral', 'neutral', 'happy', 'neutral', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'happy', 'happy', 'happy', 'sad', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'angry', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'neutral', 'happy', 'happy', 'happy', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'neutral', 'happy', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'sad', 'fear', 'surprise', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'sad', 'happy', 'happy', 'sad', 'neutral', 'fear', 'fear', 'neutral', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'neutral', 'neutral', 'neutral', 'sad', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'sad', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'sad', 'neutral', 'neutral', 'neutral', 'sad', 'sad', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'angry', 'sad', 'neutral', 'sad', 'neutral', 'fear', 'fear', 'happy', 'neutral', 'fear', 'happy', 'sad', 'happy', 'happy', 'happy', 'happy', 'sad', 'neutral', 'neutral', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'fear', 'happy', 'happy', 'happy', 'fear', 'happy', 'fear', 'happy', 'happy', 'fear', 'fear', 'fear', 'happy', 'happy', 'happy', 'fear', 'happy', 'happy', 'sad', 'happy', 'happy', 'sad', 'sad', 'sad', 'sad', 'happy', 'sad', 'happy', 'happy', 'fear', 'fear', 'happy', 'sad', 'sad', 'sad', 'happy', 'sad', 'happy', 'happy', 'neutral', 'happy', 'sad', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'neutral', 'sad', 'neutral', 'neutral', 'happy', 'neutral', 'happy', 'happy', 'happy', 'happy', 'fear', 'happy', 'neutral', 'fear', 'happy', 'happy', 'happy', 'neutral', 'happy', 'happy', 'happy', 'happy', 'happy', 'happy', 'neutral', 'sad', 'sad', 'sad', 'sad', 'sad', 'fear', 'fear', 'sad', 'sad', 'sad', 'sad', 'sad', 'sad', 'fear', 'fear', 'sad', 'fear', 'fear', 'fear', 'fear', 'fear', 'sad', 'sad', 'sad', 'fear']\n"
     ]
    }
   ],
   "source": [
    "# Função para reconhecimento facial e análise de emoções\n",
    "def analyze_faces(frame, front_cascade):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    front_faces = front_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    emotions = []\n",
    "    \n",
    "    faces = []\n",
    "    faces.extend(front_faces)  \n",
    "    # faces.extend(profile_faces)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Análise de emoções\n",
    "        analysis = DeepFace.analyze(roi, actions=['emotion'], enforce_detection=False)\n",
    "        emotions.append(analysis[0]['dominant_emotion'])\n",
    "        \n",
    "        # Marcação da face no vídeo\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, analysis[0]['dominant_emotion'], (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    return frame, faces, emotions\n",
    "\n",
    "# Função para detecção de anomalias nos movimentos (base simples usando diferença de frames)\n",
    "def detect_anomalies(prev_frame, curr_frame):\n",
    "    frame_diff = cv2.absdiff(curr_frame, prev_frame)\n",
    "    gray_diff = cv2.cvtColor(frame_diff, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray_diff, 25, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    anomalies = []\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 500:  # Área mínima para considerar como anômalo\n",
    "            anomalies.append(contour)\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "# Função principal para análise de vídeo\n",
    "def process_video(input_video_path, output_video_path):\n",
    "    # Inicializando o detector de rostos (Haar Cascade)\n",
    "    front_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')\n",
    "    \n",
    "    # Abrindo o vídeo\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Configurando o vídeo de saída\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, 20.0, (frame_width, frame_height))\n",
    "    \n",
    "    # Variáveis para análise\n",
    "    total_frames = 0\n",
    "    anomalies_detected = 0\n",
    "    prev_frame = None\n",
    "    emotions_data = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        total_frames += 1\n",
    "        frame_copy = frame.copy()\n",
    "        \n",
    "        # 1. Reconhecimento facial e análise de emoções\n",
    "        frame, faces, emotions = analyze_faces(frame, front_cascade)\n",
    "    \n",
    "        # 2. Detecção de anomalias\n",
    "        if prev_frame is not None:\n",
    "            anomalies = detect_anomalies(prev_frame, frame)\n",
    "            anomalies_detected += len(anomalies)\n",
    "            \n",
    "            for anomaly in anomalies:\n",
    "                cv2.drawContours(frame, [anomaly], -1, (0, 0, 255), 2)\n",
    "        \n",
    "        # Armazenando os dados das emoções\n",
    "        emotions_data.extend(emotions)\n",
    "        \n",
    "        # Salvando o frame processado no vídeo de saída\n",
    "        out.write(frame)\n",
    "        \n",
    "        prev_frame = frame_copy\n",
    "    \n",
    "    # Finalizando o vídeo\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    # Gerar relatório\n",
    "    report = {\n",
    "        \"total_frames\": total_frames,\n",
    "        \"anomalies_detected\": anomalies_detected,\n",
    "        \"emotions_data\": emotions_data\n",
    "    }\n",
    "    \n",
    "    # Salvar o relatório em JSON\n",
    "    with open('video_analysis_report.json', 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "\n",
    "    print(\"Processamento completo. Relatório gerado e vídeo de saída salvo.\")\n",
    "    return report\n",
    "\n",
    "# Execução do código\n",
    "input_video_path = 'video_analise.mp4'  # Caminho para o vídeo de entrada\n",
    "output_video_path = 'output_video.avi'  # Caminho para o vídeo de saída\n",
    "\n",
    "report = process_video(input_video_path, output_video_path)\n",
    "\n",
    "# Exibir o relatório\n",
    "print(\"Relatório:\")\n",
    "print(f\"Total de frames analisados: {report['total_frames']}\")\n",
    "print(f\"Número de anomalias detectadas: {report['anomalies_detected']}\")\n",
    "print(f\"Emoções detectadas: {report['emotions_data']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "\n",
    "# Função para reconhecimento facial e análise de emoções\n",
    "def analyze_faces(frame, front_cascade):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    front_faces = front_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    emotions = []\n",
    "    \n",
    "    faces = []\n",
    "    faces.extend(front_faces)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Análise de emoções\n",
    "        analysis = DeepFace.analyze(roi, actions=['emotion'], enforce_detection=False)\n",
    "        emotions.append(analysis[0]['dominant_emotion'])\n",
    "        \n",
    "        # Marcação da face no vídeo\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, analysis[0]['dominant_emotion'], (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    return frame, faces, emotions\n",
    "\n",
    "# Função para detecção de anomalias nos movimentos\n",
    "def detect_anomalies(prev_frame, curr_frame):\n",
    "    frame_diff = cv2.absdiff(curr_frame, prev_frame)\n",
    "    gray_diff = cv2.cvtColor(frame_diff, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray_diff, 25, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    anomalies = []\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 500:\n",
    "            anomalies.append(contour)\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "\n",
    "# Função para detectar atividades baseada na pose\n",
    "def detect_activities(video_frame):\n",
    "\n",
    "    # Função para detectar atividades\n",
    "    # Inicialização do MediaPipe\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "    activities_detected = []  # Lista para armazenar as atividades detectadas\n",
    "    current_activity = None    # Variável para armazenar a atividade atual\n",
    "\n",
    "    # Converte a imagem para o formato RGB, que é necessário para o MediaPipe\n",
    "    rgb_frame = cv2.cvtColor(video_frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb_frame)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        # Aqui você pode adicionar sua lógica para definir as atividades\n",
    "        # Por simplicidade, vamos apenas exemplificar com base na quantidade de pontos detectados\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        activity_confidence = len([lm for lm in landmarks if lm.visibility > 0.5])\n",
    "        \n",
    "        # Você pode definir atividades baseadas em posição ou condições de visibilidade\n",
    "        if activity_confidence > 10:  # Exemplo de threshold\n",
    "            current_activity = 'Atividade Detectada'  # Simples exemplo\n",
    "            activities_detected.append(current_activity)\n",
    "\n",
    "    return activities_detected, current_activity\n",
    "\n",
    "# Função principal para análise de vídeo\n",
    "def process_video(input_video_path, output_video_path):\n",
    "    # Inicializando o detector de rostos\n",
    "    front_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')\n",
    "    \n",
    "    # Abrindo o vídeo\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Configurando o vídeo de saída\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, 20.0, (frame_width, frame_height))\n",
    "    \n",
    "    # Variáveis para análise\n",
    "    total_frames = 0\n",
    "    anomalies_detected = 0\n",
    "    activities_detected = 0\n",
    "    prev_frame = None\n",
    "    emotions_data = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        total_frames += 1\n",
    "        frame_copy = frame.copy()\n",
    "        \n",
    "        # 1. Reconhecimento facial e análise de emoções\n",
    "        frame, faces, emotions = analyze_faces(frame, front_cascade)\n",
    "    \n",
    "        # 2. Detecção de anomalias e atividades\n",
    "        if prev_frame is not None:\n",
    "            anomalies = detect_anomalies(prev_frame, frame)\n",
    "            anomalies_detected += len(anomalies)\n",
    "            activities, current_activity = detect_activities(frame)\n",
    "            activities_detected += len(activities)\n",
    "\n",
    "        # Armazenando os dados das emoções\n",
    "        emotions_data.extend(emotions)\n",
    "        \n",
    "        # Salvando o frame processado no vídeo de saída\n",
    "        out.write(frame)\n",
    "        \n",
    "        prev_frame = frame_copy\n",
    "\n",
    " # Finalizando o vídeo\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    # Gerar relatório\n",
    "    report = {\n",
    "        \"total_frames\": total_frames,\n",
    "        \"anomalies_detected\": anomalies_detected,\n",
    "        \"emotions_data\": emotions_data,\n",
    "        \"activities_detected\": activities_detected\n",
    "    }\n",
    "    \n",
    "    # Salvar o relatório em JSON\n",
    "    with open('video_analysis_report.json', 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "\n",
    "    print(\"Processamento completo. Relatório gerado e vídeo de saída salvo.\")\n",
    "    return report\n",
    "\n",
    "# Execução do código\n",
    "input_video_path = 'video_analise.mp4'  # Caminho para o vídeo de entrada\n",
    "output_video_path = 'output_video.avi'  # Caminho para o vídeo de saída\n",
    "\n",
    "report = process_video(input_video_path, output_video_path)\n",
    "\n",
    "# Exibir o relatório\n",
    "print(\"Relatório:\")\n",
    "print(f\"Total de frames analisados: {report['total_frames']}\")\n",
    "print(f\"Número de anomalias detectadas: {report['anomalies_detected']}\")\n",
    "print(f\"Emoções detectadas: {report['emotions_data']}\")\n",
    "print(f\"Atividades detectadas: {report['activity_detected']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeepFace\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Função para reconhecimento facial e análise de emoções\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\deepface\\DeepFace.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# package dependencies\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommons\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m package_utils, folder_utils\n",
      "File \u001b[1;32mc:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32mc:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[0m\n\u001b[0;32m     86\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import json\n",
    "from deepface import DeepFace\n",
    "import mediapipe as mp\n",
    "\n",
    "# Função para reconhecimento facial e análise de emoções\n",
    "def analyze_faces(frame, front_cascade):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    front_faces = front_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    emotions = []\n",
    "    \n",
    "    faces = []\n",
    "    faces.extend(front_faces)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Análise de emoções\n",
    "        analysis = DeepFace.analyze(roi, actions=['emotion'], enforce_detection=False)\n",
    "        emotions.append(analysis[0]['dominant_emotion'])\n",
    "        \n",
    "        # Marcação da face no vídeo\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, analysis[0]['dominant_emotion'], (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    return frame, faces, emotions\n",
    "\n",
    "# Função para detecção de anomalias nos movimentos\n",
    "def detect_anomalies(prev_frame, curr_frame):\n",
    "    frame_diff = cv2.absdiff(curr_frame, prev_frame)\n",
    "    gray_diff = cv2.cvtColor(frame_diff, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray_diff, 25, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    anomalies = []\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 500:  # Ajustar o limite conforme necessário\n",
    "            anomalies.append(contour)\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "# Função para detectar atividades baseada na pose\n",
    "def detect_activities(video_frame):\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "    activities_detected = []  # Lista para armazenar as atividades detectadas\n",
    "    current_activity = \"Inativo\"  # Inicia como inativo\n",
    "\n",
    "    # Converte a imagem para o formato RGB que é necessário para o MediaPipe\n",
    "    rgb_frame = cv2.cvtColor(video_frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb_frame)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        activity_confidence = len([lm for lm in landmarks if lm.visibility > 0.5])\n",
    "        \n",
    "        # Definindo atividades baseadas na visibilidade dos landmarks\n",
    "        if activity_confidence > 10:  # Exemplo de threshold\n",
    "            current_activity = 'Atividade Detectada'  # Simples exemplo\n",
    "            activities_detected.append(current_activity)\n",
    "\n",
    "    return activities_detected, current_activity\n",
    "\n",
    "# Função principal para análise de vídeo\n",
    "def process_video(input_video_path, output_video_path):\n",
    "    # Inicializando o detector de rostos\n",
    "    front_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')\n",
    "    \n",
    "    # Abrindo o vídeo\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Configurando o vídeo de saída\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, 20.0, (frame_width, frame_height))\n",
    "    \n",
    "    # Variáveis para análise\n",
    "    total_frames = 0\n",
    "    anomalies_detected = 0\n",
    "    activities_detected = 0\n",
    "    prev_frame = None\n",
    "    emotions_data = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        total_frames += 1\n",
    "        frame_copy = frame.copy()\n",
    "        \n",
    "        # 1. Reconhecimento facial e análise de emoções\n",
    "        frame, faces, emotions = analyze_faces(frame, front_cascade)\n",
    "    \n",
    "        # 2. Detecção de anomalias e atividades\n",
    "        if prev_frame is not None:\n",
    "            anomalies = detect_anomalies(prev_frame, frame)\n",
    "            # Contar anomalias detectadas\n",
    "            anomalies_detected += len(anomalies)\n",
    "            \n",
    "            # Chamando a função para detectar atividades\n",
    "            activities, current_activity = detect_activities(frame)\n",
    "            activities_detected += len(activities)\n",
    "\n",
    "        # Armazenando os dados das emoções\n",
    "        emotions_data.extend(emotions)\n",
    "        \n",
    "        # Salvando o frame processado no vídeo de saída\n",
    "        out.write(frame)\n",
    "        \n",
    "        # Atualiza o frame anterior\n",
    "        prev_frame = frame_copy\n",
    "\n",
    "    # Finalizando o vídeo\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    # Gerar relatório\n",
    "    report = {\n",
    "        \"total_frames\": total_frames,\n",
    "        \"anomalies_detected\": anomalies_detected,\n",
    "        \"emotions_data\": emotions_data,\n",
    "        \"activities_detected\": activities_detected\n",
    "    }\n",
    "    \n",
    "    # Salvar o relatório em JSON\n",
    "    with open('video_analysis_report.json', 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "\n",
    "    print(\"Processamento completo. Relatório gerado e vídeo de saída salvo.\")\n",
    "    return report\n",
    "\n",
    "# Execução do código\n",
    "input_video_path = 'video_analise.mp4'  # Caminho para o vídeo de entrada\n",
    "output_video_path = 'output_video.avi'  # Caminho para o vídeo de saída\n",
    "\n",
    "report = process_video(input_video_path, output_video_path)\n",
    "\n",
    "# Exibir o relatório\n",
    "print(\"Relatório:\")\n",
    "print(f\"Total de frames analisados: {report['total_frames']}\")\n",
    "print(f\"Número de anomalias detectadas: {report['anomalies_detected']}\")\n",
    "print(f\"Emoções detectadas: {report['emotions_data']}\")\n",
    "print(f\"Número de atividades detectadas: {report['activities_detected']}\")\n",
    "\n",
    "pip install tensorflow --upgrade --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
