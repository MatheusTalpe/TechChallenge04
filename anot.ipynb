{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para reconhecimento facial e análise de emoções\n",
    "def analyze_faces(frame, front_cascade):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    front_faces = front_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    emotions = []\n",
    "    \n",
    "    faces = []\n",
    "    faces.extend(front_faces)  \n",
    "    # faces.extend(profile_faces)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Análise de emoções\n",
    "        analysis = DeepFace.analyze(roi, actions=['emotion'], enforce_detection=False)\n",
    "        emotions.append(analysis[0]['dominant_emotion'])\n",
    "        \n",
    "        # Marcação da face no vídeo\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, analysis[0]['dominant_emotion'], (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    return frame, faces, emotions\n",
    "\n",
    "# Função para detecção de anomalias nos movimentos (base simples usando diferença de frames)\n",
    "def detect_anomalies(prev_frame, curr_frame):\n",
    "    frame_diff = cv2.absdiff(curr_frame, prev_frame)\n",
    "    gray_diff = cv2.cvtColor(frame_diff, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray_diff, 25, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    anomalies = []\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 500:  # Área mínima para considerar como anômalo\n",
    "            anomalies.append(contour)\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "# Função principal para análise de vídeo\n",
    "def process_video(input_video_path, output_video_path):\n",
    "    # Inicializando o detector de rostos (Haar Cascade)\n",
    "    front_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')\n",
    "    \n",
    "    # Abrindo o vídeo\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Configurando o vídeo de saída\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, 20.0, (frame_width, frame_height))\n",
    "    \n",
    "    # Variáveis para análise\n",
    "    total_frames = 0\n",
    "    anomalies_detected = 0\n",
    "    prev_frame = None\n",
    "    emotions_data = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        total_frames += 1\n",
    "        frame_copy = frame.copy()\n",
    "        \n",
    "        # 1. Reconhecimento facial e análise de emoções\n",
    "        frame, faces, emotions = analyze_faces(frame, front_cascade)\n",
    "    \n",
    "        # 2. Detecção de anomalias\n",
    "        if prev_frame is not None:\n",
    "            anomalies = detect_anomalies(prev_frame, frame)\n",
    "            anomalies_detected += len(anomalies)\n",
    "            \n",
    "            for anomaly in anomalies:\n",
    "                cv2.drawContours(frame, [anomaly], -1, (0, 0, 255), 2)\n",
    "        \n",
    "        # Armazenando os dados das emoções\n",
    "        emotions_data.extend(emotions)\n",
    "        \n",
    "        # Salvando o frame processado no vídeo de saída\n",
    "        out.write(frame)\n",
    "        \n",
    "        prev_frame = frame_copy\n",
    "    \n",
    "    # Finalizando o vídeo\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    # Gerar relatório\n",
    "    report = {\n",
    "        \"total_frames\": total_frames,\n",
    "        \"anomalies_detected\": anomalies_detected,\n",
    "        \"emotions_data\": emotions_data\n",
    "    }\n",
    "    \n",
    "    # Salvar o relatório em JSON\n",
    "    with open('video_analysis_report.json', 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "\n",
    "    print(\"Processamento completo. Relatório gerado e vídeo de saída salvo.\")\n",
    "    return report\n",
    "\n",
    "# Execução do código\n",
    "input_video_path = 'video_analise.mp4'  # Caminho para o vídeo de entrada\n",
    "output_video_path = 'output_video.avi'  # Caminho para o vídeo de saída\n",
    "\n",
    "report = process_video(input_video_path, output_video_path)\n",
    "\n",
    "# Exibir o relatório\n",
    "print(\"Relatório:\")\n",
    "print(f\"Total de frames analisados: {report['total_frames']}\")\n",
    "print(f\"Número de anomalias detectadas: {report['anomalies_detected']}\")\n",
    "print(f\"Emoções detectadas: {report['emotions_data']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
